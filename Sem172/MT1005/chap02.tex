\chapter{Partial Derivatives}

\hi{Function of two variables}
    \hii{Definition}
        \par A function of two variables is a rule that assigns
        to each ordered pair of real numbers $(x, y)$ in a set a unique real number
        denoted by $f(x, y)$. The set $D$ is the \textbf{domain} of $f$ and its
        \textbf{range} is the set of values that $f$ takes on.
        \par We often write:
        \begin{eqbox}
            z = f(x, y)
        \end{eqbox}
        where:
        \begin{itemize}
            \item $x$, $y$: independent variables
            \item $z$: dependent variable
        \end{itemize}
    \hii{Domain}
        \par Given the function: $f(x, y)$. The domain of $f$ is the
        \textbf{Cartesian product} of the set $D_{x}$ and the set $D_{y}$, where
        $D_{x}$ and $D_{y}$ are the set of all possible values of $x$ and $y$,
        respectively. This is true for any multivariable function.
        \begin{eqbox}
            D = D_{x} \times D_{y} \subseteq \mathbb{R}^{2}
        \end{eqbox}
    \hii{Graph}
        \par If $f$ is a function of two variables with domain $D$, then the graph
        of $f$ is the set of all points $(x, y, z)$ in $\mathbb{R}^{3}$ such that
        $z = f(x, y)$ and $(x, y) \in D$.
    \hii{Level Curves}
        \par The \textbf{level curves} of a function $f$ of two variables are the curves
        with equations $f(x, y) = k$, where $k$ is a constant (in the range of $f$).
        \par The graph of level curves is called the \textbf{contour graph}.
        \par To construct a contour graph, the set of $k$ is required.

\hi{Limits and Continuity}
    \hiiBEGIN{Limits}
        \hiii{Definition}
            \par Let $f$ be a function of two variables whose domain $D$ includes points
            arbitrarily close to $(a, b)$. Then we say that the \textbf{limit} of
            $f(x, y)$ as $(x, y)$ approaches $(a, b)$ is $L$ and we write
            \begin{eqbox}
                \lim_{(x, y) \to (a, b)} f(x, y) = L
            \end{eqbox}
            if for every number $\epsilon > 0$ there is a corresponding number
            $\delta > 0$ such that:
            \begin{center}
                if $(x, y) \in D$ and $0 < \sqrt{(x - a)^{2} - (y - b)^{2})} < \delta$, then
                $|f(x, y) - L| < \epsilon$
            \end{center}
        \hiii{Existence of limit}
            \par For a function of one variable
            \begin{center}
                If $\lim_{x \to a^{-}} f(x) \neq \lim_{x \to a^{+}}$, then
                $\lim_{x \to a} f(x)$ does not exists.
            \end{center}
            \par For a multivariable function, the limit at one point can be approaches
            from infinitely many directions. If there \textbf{exists two different paths}
            of approach along which the function $f(x, y)$ has different limits, then the
            limit at that point does not exists.
    \hiiEND
    \hii{Continuity}
        \par A function $f$ of two variables is called \textbf{continuous} at $(a, b)$ if:
        \begin{eqbox}
            \lim_{(x, y) \to (a, b)} f(x, y) = f(a, b)
        \end{eqbox}
        \par We say $f$ is continuous on $D$ if $f$ is continuous at every point $(a, b)$ in $D$.

\hi{Partial Derivatives}
    \hii{Definition}
        \par Given the function $f(x, y)$.
        \par The \textbf{partial derivative} of $f$ with respect to $x$ at $(a, b)$, denoted by
        $f_{y}(a, b)$, is obtained by keeping $y$ fixed $(y = b)$ and finding the ordinary
        derivative at $a$ of the function $G(x) = f(x, b)$.
        \begin{eqbox}
            f_{x} (x, y) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x, y) - f(x, y)}{\Delta x} \\
            f_{y} (x, y) = \lim_{\Delta y \to 0} \frac{f(x, y + \Delta y) - f(x, y)}{\Delta y}
        \end{eqbox}
    \hii{Notations}
        \begin{eqbox}
            f_{x} (x, y) = f_{x} = \pd{f}{x} = \pd{}{x} f(x, y) = \pd{z}{x} = D_{1}f = D_{x}f \\
            f_{y} (x, y) = f_{y} = \pd{f}{y} = \pd{}{y} f(x, y) = \pd{z}{y} = D_{2}f = D_{y}f
        \end{eqbox}
    \hii{Rule for Finding Partial Derivatives}
        \par Given the function $z = f(x, y)$.
        \begin{itemize}
            \item To find $f_{x}$, regard $y$ as a constant and differentiate $f(x, y)$
                with respect to $x$.
            \item To find $f_{y}$, regard $x$ as a constant and differentiate $f(x, y)$
                with respect to $y$.
        \end{itemize}
    \hii{Higher Derivatives}
        \begin{alignat*}{4}
            (f_{x})_{x} (x, y) = f_{xx} = f_{11}
                &= \pd{}{x}\bigg(\pd{f}{x}\bigg)
                &= \pd{^{2}f}{x^{2}}
                &= \pd{^{2}z}{x^{2}} \\
            (f_{x})_{y} (x, y) = f_{xy} = f_{12}
                &= \pd{}{y}\bigg(\pd{f}{x}\bigg)
                &= \pdd{^{2}f}{y}{x}
                &= \pdd{^{2}z}{y}{x} \\
            (f_{y})_{x} (x, y) = f_{yx} = f_{21}
                &= \pd{}{x}\bigg(\pd{f}{y}\bigg)
                &= \pdd{^{2}f}{x}{y}
                &= \pdd{^{2}z}{x}{y} \\
            (f_{y})_{y} (x, y) = f_{yy} = f_{22}
                &= \pd{}{y}\bigg(\pd{f}{y}\bigg)
                &= \pd{^{2}f}{y^{2}}
                &= \pd{^{2}z}{y^{2}}
        \end{alignat*}


\hi{Tangent Plane and Linear Approximation}
    \hii{Differentials}
        \par For a differentiable function of two variables, $z = f(x, y)$, we define the
        differentials $dx$ and $dy$ to be independent variables. Then the differential $dz$,
        also called the total differntial, is defined by:
        \begin{eqbox}
            dz = f_{x}(x, y) dx + f_{y}(x, y) dy = \pd{f}{x} dx + \pd{f}{y} dy
        \end{eqbox}
        \par If we take $dx = \Delta x = x - a$ and $dy = \Delta y = y - b$,
        then the differential of $z$ is:
        \begin{alignat*}{2}
            dz &= f_{x} (x_{1}, y_{1}) \Delta x + f_{y} (x_{1}, y_{1}) \Delta y \\
            &= f_{x} (x_{1}, y_{1}) (x_{2} - x_{1}) + f_{y} (x_{1}, y_{1}) (y_{2} - y_{1})
        \end{alignat*}

\hi{The Chain Rule}
    \hii{Case 1}
        \par Suppose that $z = f(x, y)$ is a differentiable function of $x$ and $y$, where
        $x = g(t)$ and $y = h(t)$ are both differentiable functions of $t$. Then $z$ is a
        differentiable function of $t$ and
        \begin{eqbox}
            \dif{z}{t} = \pd{z}{x} \dif{x}{t} + \pd{z}{y} \dif{y}{t}
        \end{eqbox}
    \hii{Case 2}
        \par Suppose that $z = f(x, y)$ is a differentiable function of $x$ and $y$, where
        $x = g(s, t)$ and $y = h(s, t)$ are both differentiable functions of $s$ and $t$.
        Then $z$ is a differentiable function of $s$ and $t$ and
        \begin{eqbox}
            \pd{z}{s} = \pd{z}{x} \dif{x}{s} + \pd{z}{y} \dif{y}{s} \\
            \pd{z}{t} = \pd{z}{x} \dif{x}{t} + \pd{z}{y} \dif{y}{t}
        \end{eqbox}

\hi{Maximum and Minimum Values}
    \hii{Definition}
        \par A function of two variables has a \textbf{local maximum} at $(a, b)$ if
        $f(x, y) \leq f(a, b)$ when $(x, y)$ is near $(a, b)$. The number $f(a, b)$ is
        called a \textbf{local maximum value}.
        \par A function of two variables has a \textbf{local minimum} at $(a, b)$ if
        $f(x, y) \geq f(a, b)$ when $(x, y)$ is near $(a, b)$. The number $f(a, b)$ is
        called a \textbf{local minimum value}.
        \par If the inequalities hold for all points $(x, y)$ in the domain of $f$, then $f$
        has an \textbf{absolute maximum} (or \textbf{absolute minimum}) at $(a, b)$.

    \hii{Theorem (Fermat theorem for multivariable function)}
        \par If $f$ has a local maximum or minimum at $(a, b)$ and the first-order partial
        derivatives of $f$ exist there, then $f_{x}(a, b) = 0$ and $f_{y} (a, b) = 0$.
        
    \hii{Second Derivatives Test}
        \par Suppose the second partial derivatives of $f$ are continuous on a disk with center
        $(a, b)$, and suppose that $f_{x}(a, b) = 0$ and $f_{y}(a, b) = 0$.
        \par Let:
        \begin{itemize}
            \item $A = f_{xx}(a, b)$
            \item $B = f_{xy}(a, b)$
            \item $C = f_{yy}(a, b)$
            \item $D = AC - B^{2}$
        \end{itemize}
        then
        \begin{itemize}
            \item $D > 0$ and $A > 0$, then $f(a, b)$ is a local minimum.
            \item $D > 0$ and $A < 0$, then $f(a, b)$ is a local maximum.
            \item $D < 0$, then $f(a, b)$ is not a local maximum or minimum. Instead, it
                is called a \textbf{saddle point}.
            \item $D = 0$, there is no conclusion can be made. $f(a, b)$ can either be a
                local maximum/minimum or a saddle point.
        \end{itemize}
        \par The formula of $D$ can be written in the form of the determinant:
        \begin{equation}
            D = 
            \begin{vmatrix}
                f_{xx} & f_{xy} \\
                f_{yx} & f_{yy}
            \end{vmatrix}
        \end{equation}

    \hii{Absolute Maximum and Minimum Values}
